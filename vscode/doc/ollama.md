# Experimental autocomplete support with Ollama running locally

## How to use

1. Install and run [Ollama](https://ollama.ai/)
2. Download Code Llama 7b: `ollama pull codellama:7b-code` or [other codellama verison](https://ollama.ai/library/codellama/tags) you prefer to use locally.
3. Update Cody's VS Code settings to use the `unstable-ollama` autocomplete provider.
4. Confirm Cody uses Ollama by looking at the Cody output channel or the autocomplete trace view (in the command palette).
